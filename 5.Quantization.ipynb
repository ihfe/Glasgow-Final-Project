{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyOt/VE1g9ex/FeX7gl6iHeS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### 📌前置工作"],"metadata":{"id":"KAffDZF2C30T"}},{"cell_type":"markdown","source":["Ⅰ、从`torchvision.datasets`中加载数据集"],"metadata":{"id":"PohmHjgdSTFX"}},{"cell_type":"code","source":["import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import time\n","import copy\n","import torch.ao.quantization.quantize_fx as quantize_fx\n","import os\n","from torch.ao.quantization import (\n","  get_default_qconfig_mapping,\n","  get_default_qat_qconfig_mapping,\n","  QConfigMapping,\n",")"],"metadata":{"id":"ZhdehUJf_ZUb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 数据预处理：resize 和 normalization\n","transform = transforms.Compose([\n","    transforms.Resize(224),  # 因为 VGG 输入是 224x224\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# 加载训练集\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n","\n","# 加载测试集\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n","n_test = len(testset)\n","example_input = (testset[0])"],"metadata":{"id":"hLuF-AJVSSxD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ⅱ、导入`Google Drive`，便于保存模型"],"metadata":{"id":"BXtCtOAYRwMF"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","import torch.nn as nn\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"-K-UEFMzR7Gd","executionInfo":{"status":"ok","timestamp":1754432004871,"user_tz":-60,"elapsed":4202,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"5b0ca0ff-4ff8-471e-abfc-c7a1fa1307a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["# basemodel.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aCY9MIAhKQcR","executionInfo":{"status":"ok","timestamp":1754429723434,"user_tz":-60,"elapsed":8,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"7f662c69-720b-4956-9b88-0a61e30a7e6b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# next(basemodel.parameters()).device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wmcYJMFTTC7a","executionInfo":{"status":"ok","timestamp":1754432030208,"user_tz":-60,"elapsed":6,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"fbe4fa39-4a02-40b2-f50f-dec6440ebe23"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["Ⅲ、定义一些预制函数"],"metadata":{"id":"awslOCbfCvjB"}},{"cell_type":"code","source":["# 用于测试模型精度以及速度的函数\n","def test(model, test_loader, debug = False):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        start_time = time.time()\n","        for data in test_loader:\n","            images, labels = data\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","        end_time = time.time()\n","    accuracy = 100 * correct / total\n","    time_cost = end_time - start_time\n","    if debug:\n","        print('Accuracy of the network on the %d test images: %.2f %%' % (n_test, accuracy))\n","        print('Time cost: %.2f s' % time_cost)\n","    return accuracy, time_cost"],"metadata":{"id":"sxJC6h5sTfO3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 用于获取模型大小的函数，单位为 MB\n","def get_model_size(model):\n","    torch.save(model.state_dict(), \"temp.pth\")\n","    size = os.path.getsize(\"temp.pth\")/1e6\n","    os.remove('temp.pth')\n","    return size"],"metadata":{"id":"QbXcORz0LiiM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 📌模型训练、保存"],"metadata":{"id":"jm1WJcGYCL13"}},{"cell_type":"markdown","source":["#### VGG11模型训练+查看大小"],"metadata":{"id":"UDgNUdyQB_xo"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# ---------- 1. 模型定义 ----------\n","# VGG11 配置\n","vgg11_cfg = [64, 'M', 128, 'M', 256, 'M', 512, 'M', 512, 'M']\n","\n","\n","class VGG11(nn.Module):\n","    def __init__(self):\n","        super(VGG11, self).__init__()\n","        self.features = self._make_layers(vgg11_cfg)\n","        self.classifier = nn.Linear(512, 10)  # CIFAR-10 has 10 classes\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 3\n","        for x in cfg:\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [\n","                    nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                    nn.BatchNorm2d(x),\n","                    nn.ReLU(inplace=True)\n","                ]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)"],"metadata":{"collapsed":true,"id":"aJuq8BAvRKer"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------- 2. 数据预处理 ----------\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform_train)\n","trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform_test)\n","testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","# ---------- 3. 初始化模型 ----------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = VGG11().to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n","\n","# ---------- 4. 训练 ----------\n","EPOCHS = 40\n","best_acc = 0.0\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in trainloader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    scheduler.step()\n","\n","    # ---------- 测试 ----------\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in testloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","    acc = 100. * correct / total\n","    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {running_loss:.3f} - Test Acc: {acc:.2f}%\")\n","\n","    # 保存最好的模型\n","    if acc > best_acc:\n","        best_acc = acc\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        # 定义保存路径\n","        save_path = \"/content/drive/MyDrive/第二次尝试/VGG11_CIFAR10.pth\"\n","        # 保存模型参数\n","        torch.save(model.state_dict(), save_path)\n","\n","print(\"Training complete. Best accuracy: {:.2f}%\".format(best_acc))"],"metadata":{"id":"U9yE-n5T_-Mq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","# 用于获取模型大小的函数，单位为 MB\n","def get_model_size(model):\n","    torch.save(model.state_dict(), \"temp.pth\")\n","    size = os.path.getsize(\"temp.pth\")/1e6\n","    os.remove('temp.pth')\n","    return size\n","print(get_model_size(model))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"199VTmvufsvL","executionInfo":{"status":"ok","timestamp":1754436314376,"user_tz":-60,"elapsed":7,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"ad5962e2-3adc-4e1f-f97d-c681fffa7acf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["15.697388\n"]}]},{"cell_type":"markdown","source":["#### VGG13模型训练+查看大小"],"metadata":{"id":"22t2hjC4B4wD"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# ---------- 1. 模型定义 ----------\n","# VGG11 配置\n","vgg13_cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']  # 正确\n","\n","\n","class VGG13(nn.Module):\n","    def __init__(self):\n","        super(VGG13, self).__init__()\n","        self.features = self._make_layers(vgg13_cfg)\n","        self.classifier = nn.Linear(512, 10)  # CIFAR-10 has 10 classes\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 3\n","        for x in cfg:\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [\n","                    nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                    nn.BatchNorm2d(x),\n","                    nn.ReLU(inplace=True)\n","                ]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)"],"metadata":{"id":"6c7qsoNRFQrT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------- 2. 数据预处理 ----------\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform_train)\n","trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform_test)\n","testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","# ---------- 3. 初始化模型 ----------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = VGG13().to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n","\n","# ---------- 4. 训练 ----------\n","EPOCHS = 40\n","best_acc = 0.0\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in trainloader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    scheduler.step()\n","\n","    # ---------- 测试 ----------\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in testloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","    acc = 100. * correct / total\n","    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {running_loss:.3f} - Test Acc: {acc:.2f}%\")\n","\n","    # 保存最好的模型\n","    if acc > best_acc:\n","        best_acc = acc\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        # 定义保存路径\n","        save_path = \"/content/drive/MyDrive/第二次尝试/VGG13_CIFAR10.pth\"\n","        # 保存模型参数\n","        torch.save(model.state_dict(), save_path)\n","\n","print(\"Training complete. Best accuracy: {:.2f}%\".format(best_acc))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"JsiCKGDIi_BS","executionInfo":{"status":"ok","timestamp":1754470804832,"user_tz":-60,"elapsed":504420,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"0e632a2f-9164-4b80-ce40-872068486dc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:03<00:00, 43.2MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/40 - Loss: 873.551 - Test Acc: 26.95%\n","Mounted at /content/drive\n","Epoch 2/40 - Loss: 640.761 - Test Acc: 46.87%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 3/40 - Loss: 493.378 - Test Acc: 52.91%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 4/40 - Loss: 383.448 - Test Acc: 69.04%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 5/40 - Loss: 311.752 - Test Acc: 70.43%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 6/40 - Loss: 267.469 - Test Acc: 74.12%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 7/40 - Loss: 239.215 - Test Acc: 73.96%\n","Epoch 8/40 - Loss: 219.855 - Test Acc: 78.80%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 9/40 - Loss: 205.450 - Test Acc: 78.80%\n","Epoch 10/40 - Loss: 190.989 - Test Acc: 80.21%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 11/40 - Loss: 181.518 - Test Acc: 78.92%\n","Epoch 12/40 - Loss: 173.327 - Test Acc: 77.63%\n","Epoch 13/40 - Loss: 165.136 - Test Acc: 79.78%\n","Epoch 14/40 - Loss: 158.119 - Test Acc: 79.00%\n","Epoch 15/40 - Loss: 152.829 - Test Acc: 79.88%\n","Epoch 16/40 - Loss: 147.945 - Test Acc: 83.87%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 17/40 - Loss: 142.248 - Test Acc: 83.14%\n","Epoch 18/40 - Loss: 137.980 - Test Acc: 83.32%\n","Epoch 19/40 - Loss: 132.445 - Test Acc: 81.48%\n","Epoch 20/40 - Loss: 133.326 - Test Acc: 86.25%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 21/40 - Loss: 129.602 - Test Acc: 82.87%\n","Epoch 22/40 - Loss: 124.986 - Test Acc: 84.91%\n","Epoch 23/40 - Loss: 124.001 - Test Acc: 85.71%\n","Epoch 24/40 - Loss: 121.477 - Test Acc: 82.58%\n","Epoch 25/40 - Loss: 118.263 - Test Acc: 83.09%\n","Epoch 26/40 - Loss: 117.200 - Test Acc: 82.93%\n","Epoch 27/40 - Loss: 116.371 - Test Acc: 84.64%\n","Epoch 28/40 - Loss: 115.860 - Test Acc: 83.04%\n","Epoch 29/40 - Loss: 112.151 - Test Acc: 85.80%\n","Epoch 30/40 - Loss: 111.315 - Test Acc: 86.23%\n","Epoch 31/40 - Loss: 109.365 - Test Acc: 86.77%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 32/40 - Loss: 107.814 - Test Acc: 85.26%\n","Epoch 33/40 - Loss: 106.759 - Test Acc: 85.50%\n","Epoch 34/40 - Loss: 105.793 - Test Acc: 85.67%\n","Epoch 35/40 - Loss: 105.352 - Test Acc: 87.57%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 36/40 - Loss: 104.709 - Test Acc: 85.20%\n","Epoch 37/40 - Loss: 105.953 - Test Acc: 85.96%\n","Epoch 38/40 - Loss: 103.086 - Test Acc: 81.99%\n","Epoch 39/40 - Loss: 101.368 - Test Acc: 84.99%\n","Epoch 40/40 - Loss: 102.281 - Test Acc: 84.69%\n","Training complete. Best accuracy: 87.57%\n"]}]},{"cell_type":"code","source":["import os\n","# 用于获取模型大小的函数，单位为 MB\n","def get_model_size(model):\n","    torch.save(model.state_dict(), \"temp.pth\")\n","    size = os.path.getsize(\"temp.pth\")/1e6\n","    os.remove('temp.pth')\n","    return size\n","print(get_model_size(model))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOPlGrf5o9Ze","executionInfo":{"status":"ok","timestamp":1754471336330,"user_tz":-60,"elapsed":62,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"daa3438c-0708-4c97-def6-af148c4a600f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["37.707034\n"]}]},{"cell_type":"markdown","source":["#### VGG16模型训练+查看大小"],"metadata":{"id":"5zpGjwVbBg7Q"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# ---------- 1. 模型定义 ----------\n","# VGG11 配置\n","vgg16_cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n","\n","\n","class VGG16(nn.Module):\n","    def __init__(self):\n","        super(VGG16, self).__init__()\n","        self.features = self._make_layers(vgg16_cfg)\n","        self.classifier = nn.Linear(512, 10)  # CIFAR-10 has 10 classes\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 3\n","        for x in cfg:\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [\n","                    nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                    nn.BatchNorm2d(x),\n","                    nn.ReLU(inplace=True)\n","                ]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)"],"metadata":{"id":"hOa8RQ63FN2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------- 2. 数据预处理 ----------\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform_train)\n","trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform_test)\n","testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","# ---------- 3. 初始化模型 ----------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = VGG16().to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n","\n","# ---------- 4. 训练 ----------\n","EPOCHS = 40\n","best_acc = 0.0\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in trainloader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    scheduler.step()\n","\n","    # ---------- 测试 ----------\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in testloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","    acc = 100. * correct / total\n","    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {running_loss:.3f} - Test Acc: {acc:.2f}%\")\n","\n","    # 保存最好的模型\n","    if acc > best_acc:\n","        best_acc = acc\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        # 定义保存路径\n","        save_path = \"/content/drive/MyDrive/第二次尝试/VGG16_CIFAR10.pth\"\n","        # 保存模型参数\n","        torch.save(model.state_dict(), save_path)\n","\n","print(\"Training complete. Best accuracy: {:.2f}%\".format(best_acc))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"YVuMdPdokuGi","executionInfo":{"status":"ok","timestamp":1754471824324,"user_tz":-60,"elapsed":467600,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"16baa33e-195a-4237-db82-06c1d9fcfe8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40 - Loss: 936.768 - Test Acc: 21.27%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 2/40 - Loss: 742.455 - Test Acc: 32.46%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 3/40 - Loss: 628.141 - Test Acc: 44.63%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 4/40 - Loss: 507.688 - Test Acc: 53.15%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 5/40 - Loss: 407.624 - Test Acc: 55.21%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 6/40 - Loss: 340.069 - Test Acc: 63.81%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 7/40 - Loss: 297.143 - Test Acc: 72.32%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 8/40 - Loss: 265.234 - Test Acc: 72.02%\n","Epoch 9/40 - Loss: 240.578 - Test Acc: 72.05%\n","Epoch 10/40 - Loss: 220.932 - Test Acc: 79.51%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 11/40 - Loss: 206.863 - Test Acc: 74.46%\n","Epoch 12/40 - Loss: 195.284 - Test Acc: 80.67%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 13/40 - Loss: 185.681 - Test Acc: 79.52%\n","Epoch 14/40 - Loss: 176.904 - Test Acc: 83.49%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 15/40 - Loss: 168.733 - Test Acc: 82.66%\n","Epoch 16/40 - Loss: 163.733 - Test Acc: 83.13%\n","Epoch 17/40 - Loss: 155.998 - Test Acc: 80.92%\n","Epoch 18/40 - Loss: 155.899 - Test Acc: 76.40%\n","Epoch 19/40 - Loss: 149.024 - Test Acc: 83.09%\n","Epoch 20/40 - Loss: 143.452 - Test Acc: 82.34%\n","Epoch 21/40 - Loss: 141.444 - Test Acc: 81.10%\n","Epoch 22/40 - Loss: 139.865 - Test Acc: 76.76%\n","Epoch 23/40 - Loss: 137.717 - Test Acc: 85.11%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 24/40 - Loss: 135.522 - Test Acc: 85.52%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 25/40 - Loss: 132.037 - Test Acc: 83.17%\n","Epoch 26/40 - Loss: 130.319 - Test Acc: 83.63%\n","Epoch 27/40 - Loss: 126.033 - Test Acc: 86.13%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 28/40 - Loss: 127.003 - Test Acc: 84.01%\n","Epoch 29/40 - Loss: 123.009 - Test Acc: 81.65%\n","Epoch 30/40 - Loss: 123.894 - Test Acc: 82.17%\n","Epoch 31/40 - Loss: 121.411 - Test Acc: 81.51%\n","Epoch 32/40 - Loss: 120.182 - Test Acc: 79.52%\n","Epoch 33/40 - Loss: 117.890 - Test Acc: 80.35%\n","Epoch 34/40 - Loss: 121.092 - Test Acc: 82.53%\n","Epoch 35/40 - Loss: 118.651 - Test Acc: 85.77%\n","Epoch 36/40 - Loss: 116.474 - Test Acc: 84.88%\n","Epoch 37/40 - Loss: 116.032 - Test Acc: 85.21%\n","Epoch 38/40 - Loss: 115.199 - Test Acc: 81.71%\n","Epoch 39/40 - Loss: 110.414 - Test Acc: 84.63%\n","Epoch 40/40 - Loss: 111.985 - Test Acc: 82.84%\n","Training complete. Best accuracy: 86.13%\n"]}]},{"cell_type":"code","source":["import os\n","# 用于获取模型大小的函数，单位为 MB\n","def get_model_size(model):\n","    torch.save(model.state_dict(), \"temp.pth\")\n","    size = os.path.getsize(\"temp.pth\")/1e6\n","    os.remove('temp.pth')\n","    return size\n","print(get_model_size(model))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n9CaQSSWpFg5","executionInfo":{"status":"ok","timestamp":1754471824426,"user_tz":-60,"elapsed":99,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"cdc92807-728c-4709-f8c9-49b8ce806b55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["58.971868\n"]}]},{"cell_type":"markdown","source":["#### VGG19模型训练+查看大小"],"metadata":{"id":"3k_Kl5LaBoGn"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# ---------- 1. 模型定义 ----------\n","# VGG11 配置\n","vgg19_cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n","\n","class VGG19(nn.Module):\n","    def __init__(self):\n","        super(VGG19, self).__init__()\n","        self.features = self._make_layers(vgg19_cfg)\n","        self.classifier = nn.Linear(512, 10)  # CIFAR-10 has 10 classes\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 3\n","        for x in cfg:\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [\n","                    nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                    nn.BatchNorm2d(x),\n","                    nn.ReLU(inplace=True)\n","                ]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)"],"metadata":{"id":"zLoi26JBIgqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---------- 2. 数据预处理 ----------\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform_train)\n","trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform_test)\n","testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","# ---------- 3. 初始化模型 ----------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = VGG19().to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n","\n","# ---------- 4. 训练 ----------\n","EPOCHS = 40\n","best_acc = 0.0\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in trainloader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    scheduler.step()\n","\n","    # ---------- 测试 ----------\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in testloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","    acc = 100. * correct / total\n","    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {running_loss:.3f} - Test Acc: {acc:.2f}%\")\n","\n","    # 保存最好的模型\n","    if acc > best_acc:\n","        best_acc = acc\n","        from google.colab import drive\n","        drive.mount('/content/drive')\n","        # 定义保存路径\n","        save_path = \"/content/drive/MyDrive/第二次尝试/VGG19_CIFAR10.pth\"\n","        # 保存模型参数\n","        torch.save(model.state_dict(), save_path)\n","\n","print(\"Training complete. Best accuracy: {:.2f}%\".format(best_acc))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7MGKyFF_Ie02","executionInfo":{"status":"ok","timestamp":1754480212871,"user_tz":-60,"elapsed":550949,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"e704db64-03f7-443e-da04-1f53a3e3ba6f","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:13<00:00, 12.3MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/40 - Loss: 912.188 - Test Acc: 19.79%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 2/40 - Loss: 747.513 - Test Acc: 25.80%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 3/40 - Loss: 664.334 - Test Acc: 38.50%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 4/40 - Loss: 587.572 - Test Acc: 47.69%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 5/40 - Loss: 507.761 - Test Acc: 50.24%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 6/40 - Loss: 419.427 - Test Acc: 65.42%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 7/40 - Loss: 361.641 - Test Acc: 67.37%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 8/40 - Loss: 320.654 - Test Acc: 66.19%\n","Epoch 9/40 - Loss: 288.879 - Test Acc: 71.41%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 10/40 - Loss: 262.072 - Test Acc: 75.92%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 11/40 - Loss: 241.563 - Test Acc: 77.26%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 12/40 - Loss: 222.659 - Test Acc: 72.46%\n","Epoch 13/40 - Loss: 211.162 - Test Acc: 74.69%\n","Epoch 14/40 - Loss: 200.819 - Test Acc: 81.24%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 15/40 - Loss: 192.236 - Test Acc: 73.22%\n","Epoch 16/40 - Loss: 182.929 - Test Acc: 79.42%\n","Epoch 17/40 - Loss: 175.304 - Test Acc: 74.25%\n","Epoch 18/40 - Loss: 172.560 - Test Acc: 76.05%\n","Epoch 19/40 - Loss: 164.668 - Test Acc: 82.45%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 20/40 - Loss: 161.812 - Test Acc: 78.42%\n","Epoch 21/40 - Loss: 160.920 - Test Acc: 80.60%\n","Epoch 22/40 - Loss: 158.591 - Test Acc: 80.30%\n","Epoch 23/40 - Loss: 151.708 - Test Acc: 82.80%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 24/40 - Loss: 151.205 - Test Acc: 82.42%\n","Epoch 25/40 - Loss: 147.428 - Test Acc: 76.45%\n","Epoch 26/40 - Loss: 145.118 - Test Acc: 83.31%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 27/40 - Loss: 144.462 - Test Acc: 80.82%\n","Epoch 28/40 - Loss: 140.468 - Test Acc: 82.66%\n","Epoch 29/40 - Loss: 136.965 - Test Acc: 85.21%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 30/40 - Loss: 136.321 - Test Acc: 80.88%\n","Epoch 31/40 - Loss: 135.612 - Test Acc: 83.87%\n","Epoch 32/40 - Loss: 136.668 - Test Acc: 82.30%\n","Epoch 33/40 - Loss: 131.858 - Test Acc: 85.95%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 34/40 - Loss: 131.896 - Test Acc: 85.17%\n","Epoch 35/40 - Loss: 127.588 - Test Acc: 82.47%\n","Epoch 36/40 - Loss: 126.921 - Test Acc: 84.53%\n","Epoch 37/40 - Loss: 129.136 - Test Acc: 83.27%\n","Epoch 38/40 - Loss: 127.496 - Test Acc: 80.17%\n","Epoch 39/40 - Loss: 126.203 - Test Acc: 85.96%\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 40/40 - Loss: 122.453 - Test Acc: 84.67%\n","Training complete. Best accuracy: 85.96%\n"]}]},{"cell_type":"markdown","source":["### 📌训练结束，开始量化"],"metadata":{"id":"Q76CT-RD_Mrc"}},{"cell_type":"markdown","source":["#### vgg11"],"metadata":{"id":"dtfBlaU_FUv2"}},{"cell_type":"code","source":["# 加载 CIFAR10 数据集\n","batch_size = 64\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='/bohr/cifar10-h7hf/v2', train=True, download=True, transform=transform_train)\n","train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","\n","testset = torchvision.datasets.CIFAR10(root='/bohr/cifar10-h7hf/v2', train=False, download=True, transform=transform_test)\n","test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n","n_test = len(testset)\n","example_input = (testset[0])"],"metadata":{"id":"FBsN7N5RE8AE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 加载预训练模型\n","model_fp = VGG11()\n","model_fp.load_state_dict(torch.load('/content/drive/MyDrive/第二次尝试/VGG11_CIFAR10.pth', map_location=torch.device('cpu')))"],"metadata":{"id":"wRig00oN_yEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_to_quantize_ptsq = copy.deepcopy(model_fp)\n","model_to_quantize_ptsq.eval()\n","\n","qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n","model_prepared_ptsq = quantize_fx.prepare_fx(model_to_quantize_ptsq, qconfig_mapping, example_input)\n","# 这里使用全部的训练数据来对模型进行校准\n","# 注意：校准并不是训练，所以我们不需要对模型做反向传播\n","model_prepared_ptsq.eval()\n","with torch.no_grad():\n","    for images, labels in train_loader:\n","        model_prepared_ptsq(images)\n","\n","model_quantized_ptsq = quantize_fx.convert_fx(model_prepared_ptsq)"],"metadata":{"id":"CBSc94UaAZBk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 首先我们将原始模型复制一份，以防止影响原始模型\n","model_to_quantize_ptdq = copy.deepcopy(model_fp)\n","# 然后我们将模型设置为 eval 模式，因为量化时和量化后我们都不会对模型做任何的训练\n","model_to_quantize_ptdq.eval()\n","\n","# 量化模式的设置，这里我们使用的是动态量化配置\n","qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\n","\n","# 样例输入，用于推断模型的输出形状\n","example_input = (testset[0])\n","\n","# 准备并量化模型\n","model_prepared_ptdq = quantize_fx.prepare_fx(model_to_quantize_ptdq, qconfig_mapping, example_input)\n","model_quantized_ptdq = quantize_fx.convert_fx(model_prepared_ptdq)"],"metadata":{"id":"gD_KYXzqCFG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_size = []\n","model_time_cost = []\n","model_accuracy = []\n","# 测试 PTSQ 模型\n","print('PTSQ model:')\n","accuracy, time_cost = test(model_quantized_ptsq, test_loader)\n","model_size.append(get_model_size(model_quantized_ptsq))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NbqtH7NAbth","executionInfo":{"status":"ok","timestamp":1754477594052,"user_tz":-60,"elapsed":3604,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"0694c334-5143-472a-9a87-39bf4f4a7d51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PTSQ model:\n","Accuracy: 68.71%, Time cost: 3.56 s, Size: 3.93 MB\n"]}]},{"cell_type":"code","source":["# 测试原始模型\n","print('Original model:')\n","accuracy, time_cost = test(model_fp, test_loader)\n","model_size.append(get_model_size(model_fp))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Y-lGsyKAxyy","executionInfo":{"status":"ok","timestamp":1754477639824,"user_tz":-60,"elapsed":6862,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"a693958f-5f25-4843-fe27-132f175890a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original model:\n","Accuracy: 68.81%, Time cost: 6.84 s, Size: 15.70 MB\n"]}]},{"cell_type":"code","source":["# 测试 PTDQ 模型\n","print('PTDQ model:')\n","accuracy, time_cost = test(model_quantized_ptdq, test_loader)\n","model_size.append(get_model_size(model_quantized_ptdq))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZeZuTnUBByi","executionInfo":{"status":"ok","timestamp":1754477932246,"user_tz":-60,"elapsed":7436,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"adf890aa-e7e3-4cee-e009-2878aeaf3460"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PTDQ model:\n","Accuracy: 68.81%, Time cost: 7.33 s, Size: 15.65 MB\n"]}]},{"cell_type":"markdown","source":["#### VGG13"],"metadata":{"id":"DKS1Vz9gFEfr"}},{"cell_type":"code","source":["# 加载预训练模型\n","model_fp = VGG13()\n","model_fp.load_state_dict(torch.load('/content/drive/MyDrive/第二次尝试/VGG13_CIFAR10.pth', map_location=torch.device('cpu')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cDpydnmmCJDl","executionInfo":{"status":"ok","timestamp":1754478790527,"user_tz":-60,"elapsed":3621,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"b644ed25-d39e-4f9f-ec64-874c7d66bc4d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["model_to_quantize_ptsq = copy.deepcopy(model_fp)\n","model_to_quantize_ptsq.eval()\n","\n","qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n","model_prepared_ptsq = quantize_fx.prepare_fx(model_to_quantize_ptsq, qconfig_mapping, example_input)\n","# 这里使用全部的训练数据来对模型进行校准\n","# 注意：校准并不是训练，所以我们不需要对模型做反向传播\n","model_prepared_ptsq.eval()\n","with torch.no_grad():\n","    for images, labels in train_loader:\n","        model_prepared_ptsq(images)\n","\n","model_quantized_ptsq = quantize_fx.convert_fx(model_prepared_ptsq)"],"metadata":{"id":"TwDbnj_5FJ-v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 首先我们将原始模型复制一份，以防止影响原始模型\n","model_to_quantize_ptdq = copy.deepcopy(model_fp)\n","# 然后我们将模型设置为 eval 模式，因为量化时和量化后我们都不会对模型做任何的训练\n","model_to_quantize_ptdq.eval()\n","\n","# 量化模式的设置，这里我们使用的是动态量化配置\n","qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\n","\n","# 样例输入，用于推断模型的输出形状\n","example_input = (testset[0])\n","\n","# 准备并量化模型\n","model_prepared_ptdq = quantize_fx.prepare_fx(model_to_quantize_ptdq, qconfig_mapping, example_input)\n","model_quantized_ptdq = quantize_fx.convert_fx(model_prepared_ptdq)"],"metadata":{"id":"TVcNKLvJFck-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_size = []\n","model_time_cost = []\n","model_accuracy = []\n","\n","# 测试原始模型\n","print('Original model:')\n","accuracy, time_cost = test(model_fp, test_loader)\n","model_size.append(get_model_size(model_fp))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')\n","\n","# 测试 PTDQ 模型\n","print('PTDQ model:')\n","accuracy, time_cost = test(model_quantized_ptdq, test_loader)\n","model_size.append(get_model_size(model_quantized_ptdq))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')\n","\n","# 测试 PTSQ 模型\n","print('PTSQ model:')\n","accuracy, time_cost = test(model_quantized_ptsq, test_loader)\n","model_size.append(get_model_size(model_quantized_ptsq))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5X1VH5mfFijE","executionInfo":{"status":"ok","timestamp":1754478945529,"user_tz":-60,"elapsed":34205,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"587c3253-ec6b-49aa-afcd-ba671d5c7e86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original model:\n","Accuracy: 65.14%, Time cost: 14.73 s, Size: 37.71 MB\n","PTDQ model:\n","Accuracy: 65.13%, Time cost: 14.50 s, Size: 37.63 MB\n","PTSQ model:\n","Accuracy: 65.03%, Time cost: 4.80 s, Size: 9.43 MB\n"]}]},{"cell_type":"markdown","source":["#### VGG16"],"metadata":{"id":"ZF7ehiZXFlid"}},{"cell_type":"code","source":["# 加载预训练模型\n","model_fp = VGG16()\n","model_fp.load_state_dict(torch.load('/content/drive/MyDrive/第二次尝试/VGG16_CIFAR10.pth', map_location=torch.device('cpu')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zr0h_AFlFnQP","executionInfo":{"status":"ok","timestamp":1754478991710,"user_tz":-60,"elapsed":4286,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"5e70c9c5-191b-4955-8705-df3a3da2f54b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["model_to_quantize_ptsq = copy.deepcopy(model_fp)\n","model_to_quantize_ptsq.eval()\n","\n","qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n","model_prepared_ptsq = quantize_fx.prepare_fx(model_to_quantize_ptsq, qconfig_mapping, example_input)\n","# 这里使用全部的训练数据来对模型进行校准\n","# 注意：校准并不是训练，所以我们不需要对模型做反向传播\n","model_prepared_ptsq.eval()\n","with torch.no_grad():\n","    for images, labels in train_loader:\n","        model_prepared_ptsq(images)\n","\n","model_quantized_ptsq = quantize_fx.convert_fx(model_prepared_ptsq)"],"metadata":{"id":"PwBUuBTOFrvU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 首先我们将原始模型复制一份，以防止影响原始模型\n","model_to_quantize_ptdq = copy.deepcopy(model_fp)\n","# 然后我们将模型设置为 eval 模式，因为量化时和量化后我们都不会对模型做任何的训练\n","model_to_quantize_ptdq.eval()\n","\n","# 量化模式的设置，这里我们使用的是动态量化配置\n","qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\n","\n","# 样例输入，用于推断模型的输出形状\n","example_input = (testset[0])\n","\n","# 准备并量化模型\n","model_prepared_ptdq = quantize_fx.prepare_fx(model_to_quantize_ptdq, qconfig_mapping, example_input)\n","model_quantized_ptdq = quantize_fx.convert_fx(model_prepared_ptdq)"],"metadata":{"id":"CsXznn1sFr8U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_size = []\n","model_time_cost = []\n","model_accuracy = []\n","\n","# 测试原始模型\n","print('Original model:')\n","accuracy, time_cost = test(model_fp, test_loader)\n","model_size.append(get_model_size(model_fp))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')\n","\n","# 测试 PTDQ 模型\n","print('PTDQ model:')\n","accuracy, time_cost = test(model_quantized_ptdq, test_loader)\n","model_size.append(get_model_size(model_quantized_ptdq))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')\n","\n","# 测试 PTSQ 模型\n","print('PTSQ model:')\n","accuracy, time_cost = test(model_quantized_ptsq, test_loader)\n","model_size.append(get_model_size(model_quantized_ptsq))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N3S1TBueFsGV","executionInfo":{"status":"ok","timestamp":1754479144080,"user_tz":-60,"elapsed":38083,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"07133b3a-4851-4af9-f9cd-d612d8173c07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original model:\n","Accuracy: 75.53%, Time cost: 16.27 s, Size: 58.97 MB\n","PTDQ model:\n","Accuracy: 75.56%, Time cost: 16.06 s, Size: 58.87 MB\n","PTSQ model:\n","Accuracy: 75.61%, Time cost: 5.60 s, Size: 14.75 MB\n"]}]},{"cell_type":"markdown","source":["#### vgg19"],"metadata":{"id":"YZ0EINXLI2ma"}},{"cell_type":"code","source":["# 加载预训练模型\n","model_fp = VGG19()\n","model_fp.load_state_dict(torch.load('/content/drive/MyDrive/第二次尝试/VGG19_CIFAR10.pth', map_location=torch.device('cpu')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbrE2RJyFsUF","executionInfo":{"status":"ok","timestamp":1754480761184,"user_tz":-60,"elapsed":208,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"188b2764-34c2-4204-c57f-67c8de522c7c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["model_to_quantize_ptsq = copy.deepcopy(model_fp)\n","model_to_quantize_ptsq.eval()\n","\n","qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n","model_prepared_ptsq = quantize_fx.prepare_fx(model_to_quantize_ptsq, qconfig_mapping, example_input)\n","# 这里使用全部的训练数据来对模型进行校准\n","# 注意：校准并不是训练，所以我们不需要对模型做反向传播\n","model_prepared_ptsq.eval()\n","with torch.no_grad():\n","    for images, labels in train_loader:\n","        model_prepared_ptsq(images)\n","\n","model_quantized_ptsq = quantize_fx.convert_fx(model_prepared_ptsq)"],"metadata":{"id":"_1ynxd7wI6iO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 首先我们将原始模型复制一份，以防止影响原始模型\n","model_to_quantize_ptdq = copy.deepcopy(model_fp)\n","# 然后我们将模型设置为 eval 模式，因为量化时和量化后我们都不会对模型做任何的训练\n","model_to_quantize_ptdq.eval()\n","\n","# 量化模式的设置，这里我们使用的是动态量化配置\n","qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\n","\n","# 样例输入，用于推断模型的输出形状\n","example_input = (testset[0])\n","\n","# 准备并量化模型\n","model_prepared_ptdq = quantize_fx.prepare_fx(model_to_quantize_ptdq, qconfig_mapping, example_input)\n","model_quantized_ptdq = quantize_fx.convert_fx(model_prepared_ptdq)"],"metadata":{"id":"GEprF_BzI8F7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_size = []\n","model_time_cost = []\n","model_accuracy = []\n","\n","# 测试原始模型\n","print('Original model:')\n","accuracy, time_cost = test(model_fp, test_loader)\n","model_size.append(get_model_size(model_fp))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')\n","\n","# 测试 PTDQ 模型\n","print('PTDQ model:')\n","accuracy, time_cost = test(model_quantized_ptdq, test_loader)\n","model_size.append(get_model_size(model_quantized_ptdq))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')\n","\n","# 测试 PTSQ 模型\n","print('PTSQ model:')\n","accuracy, time_cost = test(model_quantized_ptsq, test_loader)\n","model_size.append(get_model_size(model_quantized_ptsq))\n","model_time_cost.append(time_cost)\n","model_accuracy.append(accuracy)\n","print(f'Accuracy: {accuracy:.2f}%, Time cost: {time_cost:.2f} s, Size: {model_size[-1]:.2f} MB')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZKjZPIgfI9IR","executionInfo":{"status":"ok","timestamp":1754480942124,"user_tz":-60,"elapsed":48092,"user":{"displayName":"ZHE YUAN","userId":"10416376004896786598"}},"outputId":"634b19c9-2466-4c97-fd11-59a84ddb94d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original model:\n","Accuracy: 72.76%, Time cost: 20.76 s, Size: 80.24 MB\n","PTDQ model:\n","Accuracy: 72.80%, Time cost: 20.61 s, Size: 80.11 MB\n","PTSQ model:\n","Accuracy: 73.15%, Time cost: 6.47 s, Size: 20.07 MB\n"]}]},{"cell_type":"markdown","source":["【Conclusion】\n","- 使用PTSQ之后模型的大小变为原来的1/4左右，但是PTDQ之后的模型大小变化不大；\n","- PTSQ模型的推理速度大幅提高，PTDQ变化不大；\n","- 二者准确率几乎和Original model的准确率一样；\n"],"metadata":{"id":"rPPPxjLoDfCj"}}]}